{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project: A Scalable Content-Based Recommendation System for Sri Lankan Tourism\n",
        "Dataset: Sri Lanka Airbnb Listings\n",
        "#\n",
        "### Objective:\n",
        "This project implements a content-based recommendation system using a real-world\n",
        "Airbnb dataset for Sri Lanka. The system leverages Apache Spark to build a scalable\n",
        "data processing pipeline, engineer features from listing attributes, and provide\n",
        "relevant accommodation recommendations.\n"
      ],
      "metadata": {
        "id": "Vuds0z9vNbc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-a2iam6Naej",
        "outputId": "3879f0c7-d8d3-44f3-cd1a-3e88ef64732c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Section 1: Environment Setup and Spark Initialization\n",
        "# ===================================================================\n",
        "\n",
        "# First, we need to install the PySpark library in our notebook environment.\n",
        "# The '!' prefix allows us to run a shell command directly from the notebook.\n",
        "# The '-q' flag is used for a \"quiet\" installation, which reduces the amount of output.\n",
        "!pip install pyspark -q\n",
        "\n",
        "# Here, we are importing the essential classes and functions we will need from PySpark.\n",
        "# Rationale for each import:\n",
        "#   - SparkSession: This is the main entry point to all Spark functionality. We can't do anything\n",
        "#     in Spark without creating a SparkSession first.\n",
        "#   - F (pyspark.sql.functions): This module contains a large collection of built-in functions\n",
        "#     for transforming and manipulating DataFrame columns. Importing it as 'F' is a standard\n",
        "#     convention that makes the code cleaner.\n",
        "#   - FloatType: Part of pyspark.sql.types, this allows us to explicitly define a column's\n",
        "#     data type, which is crucial for ensuring data quality during transformations.\n",
        "#   - ml.feature & ml.Pipeline: These are components of Spark's Machine Learning (ML) library.\n",
        "#     We need them to build a standardized workflow (a 'Pipeline') for converting our raw\n",
        "#     data attributes into numerical feature vectors that algorithms can understand.\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# This is where we create our SparkSession object, which controls our Spark application.\n",
        "# We use the builder pattern, which is a flexible way to configure the session.\n",
        "#   - .appName(): Assigns a name to our application. This name appears in the Spark UI,\n",
        "#     making it easy to monitor our job on a cluster.\n",
        "#   - .getOrCreate(): This is a smart method that checks if a SparkSession already exists.\n",
        "#     If it does, it returns the existing one; otherwise, it creates a new one. This\n",
        "#     prevents errors if we accidentally try to create multiple sessions.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AirbnbPySparkRecommender\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"SparkSession initialized successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------\n",
        "# Section 2: Data Ingestion and Preprocessing (The ETL Process)\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# This section covers the 'Extract' and 'Transform' stages of our ETL pipeline.\n",
        "# The goal is to load the raw data and prepare it for feature engineering.\n",
        "\n",
        "# --- Stage 1: Extract ---\n",
        "\n",
        "# Action: We are ingesting the raw Airbnb dataset from the uploaded CSV file.\n",
        "# Rationale: `spark.read.csv` is the standard method for loading structured text data\n",
        "# into a distributed Spark DataFrame. We've configured it with several important options:\n",
        "#   - `header=True`: This tells Spark to use the first row of the file as the column headers.\n",
        "#   - `inferSchema=True`: Spark will automatically scan the data to guess the data type for\n",
        "#     each column (e.g., Integer, String). This is very convenient for exploration.\n",
        "#   - `multiLine=True` & `escape='\\\"'`: These options are vital for data integrity. They\n",
        "#     ensure that fields which might contain special characters or span multiple lines\n",
        "#     (like long listing names) are parsed correctly into a single field.\n",
        "file_path = \"sri_lanka_airbnb.csv\"\n",
        "print(f\"\\nIngesting data from '{file_path}'...\")\n",
        "raw_df = spark.read.csv(file_path, header=True, inferSchema=True, multiLine=True, escape='\\\"')\n",
        "\n",
        "# --- Stage 2: Transform ---\n",
        "\n",
        "# Action: Here, we select the columns relevant to our model and standardize their names.\n",
        "# Rationale: A key principle in data processing is to work only with the data you need.\n",
        "# By selecting a subset of columns, we reduce the memory footprint and improve performance.\n",
        "# We use the `.alias()` method to rename columns to a consistent `snake_case` format, which is a\n",
        "# widely adopted coding standard that improves readability.\n",
        "df = raw_df.select(\n",
        "    F.col(\"name\"),\n",
        "    F.col(\"roomType\").alias(\"room_type\"),\n",
        "    F.col(\"stars\").alias(\"star_rating\"),\n",
        "    F.col(\"numberOfGuests\").alias(\"number_of_guests\")\n",
        ")\n",
        "\n",
        "# Note on price data: The 'price' feature, while potentially valuable, is not included in the\n",
        "# current selection. If it were present as a string (e.g., '$95'), a transformation step\n",
        "# involving `regexp_replace` to remove non-numeric symbols and a `.cast()` to convert it to a\n",
        "# numeric type would be necessary before it could be used in modeling.\n",
        "\n",
        "# Action: We are removing records that have null values in critical feature columns.\n",
        "# Rationale: Missing data in key fields like `star_rating` or `room_type` can lead to errors\n",
        "# or biases in our recommendation logic. Using `.na.drop()` is a straightforward strategy to\n",
        "# ensure our dataset contains only complete, high-quality records for our chosen features.\n",
        "df = df.na.drop(subset=[\"star_rating\", \"room_type\"])\n",
        "\n",
        "\n",
        "# Action: Displaying the schema and a sample of the transformed DataFrame.\n",
        "# Rationale: It's crucial to verify the results of our transformation steps.\n",
        "#   - `printSchema()` confirms that our column names and data types are correct.\n",
        "#   - `show()` allows us to inspect a sample of the data to ensure the cleaning\n",
        "#     and selection logic worked as expected.\n",
        "print(\"\\n--- Data after cleaning and preprocessing ---\")\n",
        "df.show(5, truncate=False)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aI8MY5ON8qc",
        "outputId": "cfee6a91-77d9-4229-c93f-e6855746ca24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ingesting data from 'sri_lanka_airbnb.csv'...\n",
            "\n",
            "--- Data after cleaning and preprocessing ---\n",
            "+-------------------------------------------------+---------------------------+-----------+----------------+\n",
            "|name                                             |room_type                  |star_rating|number_of_guests|\n",
            "+-------------------------------------------------+---------------------------+-----------+----------------+\n",
            "|D'Villa Garden House                             |Private room in guest suite|4.61       |2               |\n",
            "|Suppar villa, a vacation home                    |Entire home                |4.5        |6               |\n",
            "|Brinthavanam days inn /Non Ac/Free Veg Break fast|Private room in guesthouse |4.44       |2               |\n",
            "|\"Mathura\" Home / Two rooms for rental in Jaffna  |Entire home                |4.0        |4               |\n",
            "|Tony's Garden House üè° inn                       |Private room in home       |4.08       |16              |\n",
            "+-------------------------------------------------+---------------------------+-----------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- star_rating: double (nullable = true)\n",
            " |-- number_of_guests: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# Section 3: Content-Based Feature Engineering\n",
        "# ----------------------------------------------------------------\n",
        "# Objective: The primary goal of this section is to transform the descriptive,\n",
        "# raw attributes of each listing into a standardized numerical representation,\n",
        "# known as a \"feature vector\". This vector format is essential because\n",
        "# mathematical operations, like similarity calculations, can only be\n",
        "# performed on numerical data.\n",
        "\n",
        "# Action: We are defining a multi-stage feature engineering pipeline.\n",
        "# Rationale: Using a `Pipeline` from Spark's ML library is a best practice for\n",
        "# organizing our data transformation workflow. It allows us to chain multiple\n",
        "# steps together in a specific order. This makes the code more readable,\n",
        "# reusable, and ensures that the same transformations are applied consistently,\n",
        "# which is critical for both training and later, for making predictions on new data.\n",
        "\n",
        "# --- Pipeline Stage 1: StringIndexer ---\n",
        "# Purpose: To convert our categorical feature 'room_type' (which contains text\n",
        "# like \"Private room\") into a numerical format.\n",
        "# How it works: It assigns a unique numerical index to each distinct category.\n",
        "# For example: \"Private room\" -> 0.0, \"Entire home/apt\" -> 1.0.\n",
        "room_type_indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\", handleInvalid=\"keep\")\n",
        "\n",
        "# --- Pipeline Stage 2: OneHotEncoder ---\n",
        "# Purpose: To convert the numerical indices into a more meaningful binary vector format.\n",
        "# Rationale: Simply using the indices (0.0, 1.0, etc.) could mislead an algorithm into\n",
        "# thinking there is an ordinal relationship (e.g., that 1.0 > 0.0). One-Hot Encoding\n",
        "# prevents this by creating a sparse vector where each category is a dimension.\n",
        "# For example, \"Private room\" (index 0) might become a vector like [1.0, 0.0, 0.0].\n",
        "room_type_encoder = OneHotEncoder(inputCol=\"room_type_index\", outputCol=\"room_type_vec\")\n",
        "\n",
        "# --- Pipeline Stage 3: VectorAssembler ---\n",
        "# Purpose: To gather all our feature columns into a single vector column.\n",
        "# Rationale: Spark's ML algorithms expect all features for a given data point\n",
        "# to be consolidated into a single vector. Even though we only have one feature\n",
        "# vector in this case ('room_type_vec'), this is a required and scalable step.\n",
        "final_assembler = VectorAssembler(inputCols=[\"room_type_vec\"], outputCol=\"features\")\n",
        "\n",
        "# --- Pipeline Stage 4: Normalizer ---\n",
        "# Purpose: To scale our feature vectors to have a unit length (L2 norm).\n",
        "# Rationale: This is a crucial prerequisite for calculating Cosine Similarity.\n",
        "# Cosine similarity measures the angle between two vectors, and this calculation\n",
        "# is only accurate if the vectors are normalized to the same length (a magnitude of 1).\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\", p=2.0)\n",
        "\n",
        "# Action: We now define the full pipeline by listing the stages in order.\n",
        "pipeline = Pipeline(stages=[\n",
        "    room_type_indexer, room_type_encoder,\n",
        "    final_assembler, normalizer\n",
        "])\n",
        "\n",
        "# Action: We fit the pipeline to our DataFrame and then transform the data.\n",
        "# Rationale:\n",
        "#   - `.fit(df)`: This step trains our pipeline. The `StringIndexer`, for instance,\n",
        "#     scans the 'room_type' column to learn all the unique categories and create its\n",
        "#     internal index mapping.\n",
        "#   - `.transform(df)`: This step applies the learned transformations to every row\n",
        "#     in the DataFrame, executing all stages in order and adding the new columns,\n",
        "#     including our final 'norm_features' vector.\n",
        "feature_pipeline = pipeline.fit(df)\n",
        "items_with_features = feature_pipeline.transform(df)\n",
        "\n",
        "# Action: Display the results to verify our feature engineering was successful.\n",
        "# Rationale: We inspect the output to confirm that a 'norm_features' vector has\n",
        "# been created for each listing, representing its content in a numerical format.\n",
        "print(\"\\n--- Data with final engineered feature vectors ---\")\n",
        "items_with_features.select(\"name\", \"norm_features\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3loGNHiOF9p",
        "outputId": "5fcddf94-f1d3-48a9-d038-7b693ad78357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Data with final engineered feature vectors ---\n",
            "+-------------------------------------------------+---------------+\n",
            "|name                                             |norm_features  |\n",
            "+-------------------------------------------------+---------------+\n",
            "|D'Villa Garden House                             |(62,[30],[1.0])|\n",
            "|Suppar villa, a vacation home                    |(62,[1],[1.0]) |\n",
            "|Brinthavanam days inn /Non Ac/Free Veg Break fast|(62,[9],[1.0]) |\n",
            "|\"Mathura\" Home / Two rooms for rental in Jaffna  |(62,[1],[1.0]) |\n",
            "|Tony's Garden House üè° inn                       |(62,[8],[1.0]) |\n",
            "+-------------------------------------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------\n",
        "# Section 4: Recommendation Logic Implementation\n",
        "# -----------------------------------------------------------------------\n",
        "# Objective: This section contains the core algorithm of our content-based\n",
        "# recommender. We will define a function that, given a specific item, can\n",
        "# find and rank the 'top k' most similar items from our dataset based on the\n",
        "# feature vectors we engineered in the previous section.\n",
        "\n",
        "# Action: Define a function to generate content-based recommendations.\n",
        "# Rationale: This function encapsulates the recommendation logic, making it reusable\n",
        "# and easy to test. It takes a target item's name, the full DataFrame of items\n",
        "# with their feature vectors, and the number of recommendations to return (top_k).\n",
        "def recommend_spark(item_name, items_df, top_k=5):\n",
        "    \"\"\"\n",
        "    This function implements the recommendation logic by calculating cosine similarity\n",
        "    between a target item and all other items in the dataset.\n",
        "    \"\"\"\n",
        "    # Step 1: Isolate the feature vector of the target item.\n",
        "    # We filter the DataFrame to find the specific item we want recommendations for\n",
        "    # and extract its pre-computed 'norm_features' vector. A try-except block is\n",
        "    # used for robust error handling in case the item name is not found.\n",
        "    try:\n",
        "        target_vector = items_df.filter(F.col(\"name\") == item_name).select(\"norm_features\").first()[0]\n",
        "    except (IndexError, TypeError):\n",
        "        print(f\"Error: Item with name '{item_name}' not found.\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Define a User-Defined Function (UDF) to compute similarity.\n",
        "    # Rationale: Since the feature vectors are normalized (scaled to a unit length),\n",
        "    # the cosine similarity is equivalent to their dot product. We create a UDF to\n",
        "    # apply this vector operation efficiently across all rows of our distributed DataFrame.\n",
        "    dot_product_udf = F.udf(lambda x: float(x.dot(target_vector)), \"float\")\n",
        "\n",
        "    # Step 3: Compute similarity for all items and rank them.\n",
        "    # A new 'similarity' column is created by applying our UDF to every item's\n",
        "    # feature vector. This is a highly parallelized transformation.\n",
        "    similarity_df = items_df.withColumn(\"similarity\", dot_product_udf(F.col(\"norm_features\")))\n",
        "\n",
        "    # Step 4: Sort, filter, and select the top recommendations.\n",
        "    #   - `orderBy()`: We sort the entire DataFrame by the new 'similarity' column in\n",
        "    #     descending order to bring the most similar items to the top.\n",
        "    #   - `filter()`: We must exclude the input item itself from the results, as its\n",
        "    #     similarity to itself will always be a perfect 1.0.\n",
        "    #   - `limit()`: We select only the top 'k' items from the sorted list.\n",
        "    recommendations = similarity_df.orderBy(F.col(\"similarity\").desc()) \\\n",
        "                                   .filter(F.col(\"name\") != item_name) \\\n",
        "                                   .limit(top_k)\n",
        "\n",
        "    # Finally, we select a few readable columns to present the recommendations.\n",
        "    return recommendations.select(\"name\", \"room_type\", \"star_rating\", \"number_of_guests\", \"similarity\")\n",
        "\n",
        "# Action: Execute the recommendation function to test its functionality.\n",
        "# Rationale: To demonstrate that our function works, we select a sample item from\n",
        "# our dataset and generate recommendations for it. We programmatically get the first\n",
        "# item's name to ensure the code is runnable without manual input.\n",
        "target_item_name = items_with_features.first()[\"name\"]\n",
        "print(f\"\\n--- Top 5 Recommendations for '{target_item_name}' ---\")\n",
        "recs = recommend_spark(target_item_name, items_with_features, top_k=5)\n",
        "\n",
        "# We check if the function returned a DataFrame before trying to display it.\n",
        "if recs:\n",
        "    recs.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot6wOOuROQqf",
        "outputId": "82d3bb52-f7af-4bdc-a756-439dc55860a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top 5 Recommendations for 'D'Villa Garden House' ---\n",
            "+---------------------------------------------+---------------------------+-----------+----------------+----------+\n",
            "|name                                         |room_type                  |star_rating|number_of_guests|similarity|\n",
            "+---------------------------------------------+---------------------------+-----------+----------------+----------+\n",
            "|Luxury Oceanfront private Suite              |Private room in guest suite|4.92       |2               |1.0       |\n",
            "|Into The Blue - Yoga included | Walk to Beach|Private room in guest suite|4.86       |2               |1.0       |\n",
            "|Ambalama Pavillion                           |Private room in guest suite|5.0        |2               |1.0       |\n",
            "|Sudu Neluma Home Stay (Foreigners Only)      |Private room in guest suite|4.55       |5               |1.0       |\n",
            "|Ivory Villa with a private Pool and Garden   |Private room in guest suite|4.75       |2               |1.0       |\n",
            "+---------------------------------------------+---------------------------+-----------+----------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------\n",
        "# Section 5: Offline Model Validation Framework\n",
        "# --------------------------------------------------------------------------\n",
        "# Objective: This section is dedicated to the critical process of model evaluation.\n",
        "# It is insufficient to simply build a model; we must quantitatively measure its\n",
        "# performance to understand its effectiveness. Here, we construct a harness to\n",
        "# calculate Precision@K, a standard information retrieval metric adapted for\n",
        "# assessing recommendation quality.\n",
        "\n",
        "# Action: Define a function to calculate the average Precision@K for our model.\n",
        "# Rationale: This function serves as our evaluation protocol. A key challenge in a\n",
        "# Big Data context is that evaluating a model across the entire dataset can be\n",
        "# computationally expensive. Therefore, a common academic and industry practice\n",
        "# is to estimate performance by testing on a smaller, random, and statistically\n",
        "# representative sample of the data.\n",
        "def evaluate_model_spark(items_df, top_k=5, sample_size=20):\n",
        "    \"\"\"\n",
        "    Calculates average Precision@K across a random sample of items. This metric\n",
        "    is a measure of the accuracy of the top-K recommendations.\n",
        "    \"\"\"\n",
        "    # Step 1: Sub-sampling the dataset for evaluation.\n",
        "    # Rationale: We take a small fraction of the data to create our test set.\n",
        "    # The use of a fixed 'seed' is crucial for reproducibility, ensuring that\n",
        "    # the same random sample is selected each time the experiment is run, which\n",
        "    # is a cornerstone of valid scientific methodology.\n",
        "    test_items = items_df.sample(False, 0.05, seed=42).limit(sample_size).collect()\n",
        "\n",
        "    # A simple check to ensure the sampling process yielded enough data.\n",
        "    if len(test_items) < sample_size:\n",
        "        print(f\"Warning: The evaluation sample is smaller than desired ({len(test_items)} items).\")\n",
        "\n",
        "    total_precision = 0.0\n",
        "    evaluated_count = 0\n",
        "\n",
        "    # Step 2: Iterating through the test set to calculate precision for each item.\n",
        "    for item_row in test_items:\n",
        "        item_name = item_row[\"name\"]\n",
        "\n",
        "        # Step 3: Defining the \"Ground Truth\" using a relevance proxy.\n",
        "        # Rationale: The most significant challenge in offline evaluation is the lack\n",
        "        # of explicit user feedback. To address this, we define a heuristic-based\n",
        "        # proxy for relevance. Our assumption is that `room_type` is a primary\n",
        "        # decision factor for users. Therefore, if a user is viewing a specific item,\n",
        "        # other recommended items with the same `room_type` are considered relevant.\n",
        "        # It's important to acknowledge that this is a simplification of true user preference.\n",
        "        ground_truth_room_type = item_row[\"room_type\"]\n",
        "\n",
        "        # Step 4: Generating recommendations for the item under test.\n",
        "        recommendations_df = recommend_spark(item_name, items_df, top_k=top_k)\n",
        "\n",
        "        if recommendations_df is None:\n",
        "            continue\n",
        "\n",
        "        # Step 5: Computing Precision@K for the generated recommendations.\n",
        "        # The number of \"hits\" is the count of recommended items that satisfy our\n",
        "        # ground truth criterion. Precision@K is then calculated as:\n",
        "        # (Number of Relevant Items in Top-K) / K.\n",
        "        hits = recommendations_df.filter(F.col(\"room_type\") == ground_truth_room_type).count()\n",
        "        precision = hits / top_k\n",
        "        total_precision += precision\n",
        "        evaluated_count += 1\n",
        "\n",
        "    # Step 6: Aggregating the results.\n",
        "    # Rationale: A single precision score can be noisy. By averaging the precision\n",
        "    # across our entire test sample, we obtain a more stable and generalizable\n",
        "    # estimate of the model's overall performance.\n",
        "    avg_precision = total_precision / evaluated_count if evaluated_count > 0 else 0\n",
        "    return avg_precision\n",
        "\n",
        "# Action: Execute the evaluation protocol and report the final score.\n",
        "# Rationale: This is the final step where we run our defined logic to produce a\n",
        "# single, quantitative metric. This score serves as a benchmark for our model's\n",
        "# accuracy and can be used to compare different versions of the recommendation algorithm.\n",
        "avg_precision_at_5 = evaluate_model_spark(items_with_features, top_k=5, sample_size=20)\n",
        "print(f\"\\n--- Evaluation Results ---\")\n",
        "print(f\"Average Precision@5 (based on a sample of 20 items): {avg_precision_at_5:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dYNLqX_OYkK",
        "outputId": "752ad69b-b3fa-4f3e-f258-48d639b42dab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Results ---\n",
            "Average Precision@5 (based on a sample of 20 items): 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------\n",
        "# Section 6: Shutdown\n",
        "# ----------------------------------------------------\n",
        "# Action: Stop the SparkSession.\n",
        "# Rationale: It is best practice to release the cluster resources when the\n",
        "# application is finished.\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "1gBS40yeV3My"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}